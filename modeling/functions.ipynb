{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17ad112e",
   "metadata": {},
   "source": [
    "### Functions for Modeling<a class=\"anchor\" id=\"functions\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c332028",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(df, columns=None):\n",
    "    \"\"\"\n",
    "    Selects specific columns from a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame.\n",
    "    - columns (list or None): List of column names to select. If None, all columns are selected.\n",
    "\n",
    "    Returns:\n",
    "    - DataFrame: DataFrame containing only the selected columns.\n",
    "    \"\"\"\n",
    "    if columns is None:\n",
    "        # If specific columns are not provided, select all columns\n",
    "        selected_features = df\n",
    "    else:\n",
    "        # Select the provided specific columns\n",
    "        selected_features = df[columns]\n",
    "\n",
    "    return selected_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16565e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_binary_responses(df):\n",
    "    \"\"\"\n",
    "    Preprocesses binary responses in a DataFrame by mapping labels to numerical values.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing binary response columns.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with binary responses mapped to numerical values.\n",
    "    \"\"\"\n",
    "    #  Map 'Yes' and 'No' to 1 and 0 in columns containing these labels\n",
    "    mapping_yes_no = {'Yes': 1, 'No': 0}\n",
    "    df = df.replace(mapping_yes_no)\n",
    "    \n",
    "    #  Map 'depressed' and 'not_depressed' to 1 and 0 in the 'depression_status' column\n",
    "    df['depression_status'] = df['depression_status'].map({'depressed': 1, 'not_depressed': 0})\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d80eb3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_object_columns(df):\n",
    "    \"\"\"\n",
    "    One-hot encodes categorical columns in a DataFrame, excluding the 'depression_status' column.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing categorical columns.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with one-hot encoded categorical columns.\n",
    "    \"\"\"\n",
    "    # Exclude depression_status\n",
    "    exclude_columns = ['depression_status']\n",
    "    \n",
    "    # Select object columns\n",
    "    object_columns = df.select_dtypes(include=['object']).columns.difference(exclude_columns)\n",
    "    \n",
    "    # Apply one-hot codification to selected columns\n",
    "    df_encoded = pd.get_dummies(df, columns=object_columns, prefix=object_columns)\n",
    "\n",
    "    df_encoded.info()\n",
    "    \n",
    "    return df_encoded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c003515",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_data(df):\n",
    "    return (df - np.min(df)) / (np.max(df) - np.min(df))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d88e4451",
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_data(df):\n",
    "    \"\"\"\n",
    "    Standardizes numerical features in a DataFrame, leaving the 'depression_status' column unchanged.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame containing numerical features.\n",
    "\n",
    "    Returns:\n",
    "    DataFrame: DataFrame with standardized numerical features and the 'depression_status' column.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract the 'depression_status' column\n",
    "    depression_status_column = df['depression_status']\n",
    "    \n",
    "    # Save the original indices\n",
    "    original_indices = df.index\n",
    "\n",
    "    # Select all columns except 'depression_status'\n",
    "    features = df.drop('depression_status', axis=1)\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    # Create a new DataFrame with the scaled features\n",
    "    df_scaled = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "\n",
    "    # Restore the original indices\n",
    "    df_scaled.index = original_indices\n",
    "\n",
    "    # Add the 'depression_status' column to the new scaled DataFrame\n",
    "    df_scaled['depression_status'] = depression_status_column\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ec863bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target_column='depression_status', test_size=0.3, random_state=42):\n",
    "    \"\"\"\n",
    "    Splits a DataFrame into training and test sets for machine learning.\n",
    "\n",
    "    Parameters:\n",
    "    - df (DataFrame): The input DataFrame.\n",
    "    - target_column (str, optional): The name of the target column to predict. Default is 'depression_status'.\n",
    "    - test_size (float, optional): The proportion of the dataset to include in the test split. Default is 0.3.\n",
    "    - random_state (int, optional): Seed for random number generation. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "    tuple: X_train, X_test, y_train, y_test, where\n",
    "      - X_train (DataFrame): Training features.\n",
    "      - X_test (DataFrame): Test features.\n",
    "      - y_train (Series): Training target.\n",
    "      - y_test (Series): Test target.\n",
    "    \"\"\"\n",
    "\n",
    "    # Split dataframe into training and test sets\n",
    "    X = df.drop(target_column, axis=1)\n",
    "    y = df[target_column]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f0d27b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def oversample_data(X_train, y_train, strategy='minority', random_state=42):\n",
    "    \"\"\"\n",
    "    Oversamples the minority class in the target variable.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (DataFrame): Training features.\n",
    "    - y_train (Series): Training target.\n",
    "    - strategy (str or float, optional): Strategy for resampling. Default is 'minority'.\n",
    "    - random_state (int, optional): Seed for random number generation. Default is 42.\n",
    "\n",
    "    Returns:\n",
    "    tuple: X_train_oversampled, y_train_oversampled, where\n",
    "      - X_train_oversampled (DataFrame): Oversampled training features.\n",
    "      - y_train_oversampled (Series): Oversampled training target.\n",
    "    \"\"\"\n",
    "\n",
    "    # Oversample the minority class in the target variable.\n",
    "    oversample = RandomOverSampler(sampling_strategy=strategy, random_state=random_state)\n",
    "    X_train_oversampled, y_train_oversampled = oversample.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"Before oversampling: {sorted(Counter(y_train).items())}\")\n",
    "    print(f\"After oversampling: {sorted(Counter(y_train_oversampled).items())}\")\n",
    "    \n",
    "    return X_train_oversampled, y_train_oversampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41a66e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersample_data(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Undersamples the majority class using a combination of Near-Miss and Tomek Links.\n",
    "\n",
    "    Parameters:\n",
    "    - X_train (DataFrame): Training features.\n",
    "    - y_train (Series): Training target.\n",
    "\n",
    "    Returns:\n",
    "    tuple: X_train_combined, y_train_combined, where\n",
    "      - X_train_combined (DataFrame): Undersampled training features.\n",
    "      - y_train_combined (Series): Undersampled training target.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Near-Miss to undersample the majority class \n",
    "    nm = NearMiss()\n",
    "    X_train_nm, y_train_nm = nm.fit_resample(X_train, y_train)\n",
    "    \n",
    "    # Tomek Links to delete samples close to each class\n",
    "    tl = TomekLinks()\n",
    "    X_train_tl, y_train_tl = tl.fit_resample(X_train_nm, y_train_nm)\n",
    "    \n",
    "    # Near-Miss and Tomek Links combined\n",
    "    smtl = SMOTETomek()\n",
    "    X_train_combined, y_train_combined = smtl.fit_resample(X_train, y_train)\n",
    "    \n",
    "    print(f\"Before undersampling: {sorted(Counter(y_train).items())}\")\n",
    "    print(f\"After undersampling: {sorted(Counter(y_train_combined).items())}\")\n",
    "    \n",
    "    return X_train_combined, y_train_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ada481f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_search(model, X_train, y_train, param_distributions, n_iter=10, cv=5):\n",
    "    \"\"\"\n",
    "    Performs a random search for hyperparameters for a given model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to be optimized.\n",
    "    - X_train (DataFrame): Training features.\n",
    "    - y_train (Series): Training target.\n",
    "    - param_distributions (dict): Dictionary with parameter names as keys and distributions or lists of parameters to sample from.\n",
    "    - n_iter (int, optional): Number of parameter settings that are sampled. Default is 10.\n",
    "    - cv (int, optional): Number of cross-validation folds. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    BestEstimator: Best estimator found during the random search.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Performs a random search for hyperparameters for a given model\n",
    "    search = RandomizedSearchCV(model, param_distributions, n_iter=n_iter, cv=cv)\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best hyperparameters: {search.best_params_}\")\n",
    "    print(f\"Accuracy of the best model: {search.best_estimator_.score(X_test, y_test):.2f}\")\n",
    "    \n",
    "    return search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4e65538a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(model, X_train, y_train, param_distributions, cv=5, scoring='accuracy'):\n",
    "    \"\"\"\n",
    "    Performs a grid search for hyperparameters for a given model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The machine learning model to be optimized.\n",
    "    - X_train (DataFrame): Training features.\n",
    "    - y_train (Series): Training target.\n",
    "    - param_distributions (dict): Dictionary with parameter names as keys and lists of parameters to search.\n",
    "    - cv (int, optional): Number of cross-validation folds. Default is 5.\n",
    "    - scoring (str, optional): Scoring metric for model evaluation. Default is 'accuracy'.\n",
    "\n",
    "    Returns:\n",
    "    BestEstimator: Best estimator found during the grid search.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Performs a random search for hyperparameters for a given model\n",
    "    search = GridSearchCV(model, param_distributions, cv=cv, scoring=scoring)\n",
    "    search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"Best hyperparameters: {search.best_params_}\")\n",
    "    print(f\"Accuracy of the best model: {search.best_estimator_.score(X_test, y_test):.2f}\")\n",
    "    \n",
    "    return search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5977850e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_confusion_matrix(model, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Display the confusion matrix and print True Negatives, True Positives, \n",
    "    False Positives, and False Negatives.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained machine learning model.\n",
    "    - X_test: The feature matrix of the test set.\n",
    "    - y_test: The true labels of the test set.\n",
    "    \"\"\"\n",
    "    # Make predictions using the model\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix using seaborn\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.show()\n",
    "\n",
    "    # Print True Negatives, True Positives, False Positives, and False Negatives\n",
    "    print('True Negatives: {}'.format(cm[0][0]))\n",
    "    print('True Positives: {}'.format(cm[1][1]))\n",
    "    print('False Positives: {}'.format(cm[0][1]))\n",
    "    print('False Negatives: {}'.format(cm[1][0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42a587e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_classification_report(model, X_test, y_test, title=''):\n",
    "    \"\"\"\n",
    "    Prints the classification report for a given model on the test set.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained machine learning model.\n",
    "    - X_test (DataFrame): Test features.\n",
    "    - y_test (Series): Test target.\n",
    "    - title (str, optional): Title for the classification report. Default is an empty string.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"_\"*60)\n",
    "    print(f\"\\nCLASSIFICATION REPORT FOR: {title}\")\n",
    "    print(\"_\"*60)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(classification_report(y_test, predictions, zero_division=1))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acdda810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(model, X_test, y_test, title=''):\n",
    "    \"\"\"\n",
    "    Plots the precision-recall curve for a given model on the test set.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained machine learning model.\n",
    "    - X_test (DataFrame): Test features.\n",
    "    - y_test (Series): Test target.\n",
    "    - title (str, optional): Title for the precision-recall curve. Default is an empty string.\n",
    "    \"\"\"\n",
    "    \n",
    "    predictions = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, predictions)    \n",
    "    precision, recall, _ = precision_recall_curve(y_test, predictions)\n",
    "    print(f\"AUC-PR: {acc:.2f}\")\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(recall, precision, color='b', lw=2, label='PR Curve')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title(f'PRECISION CURVE FOR: {title}')\n",
    "    plt.legend(loc='lower left')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "22e29522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_results(model, X_train, X_test, y_test, title=''):\n",
    "    \"\"\"\n",
    "    Analyzes and visualizes the results of a machine learning model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained machine learning model.\n",
    "    - X_train (DataFrame): Training features.\n",
    "    - X_test (DataFrame): Test features.\n",
    "    - y_test (Series): Test target.\n",
    "    - title (str, optional): Title for the analysis. Default is an empty string.\n",
    "    \"\"\"\n",
    "    \n",
    "    print_classification_report(model, X_test, y_test, title=title)\n",
    "    plot_precision_recall_curve(model, X_test, y_test, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d9e80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_explanation(model, X_test, y_test, start_index=0, end_index=None):\n",
    "    \"\"\"\n",
    "    Generates and displays LIME explanations for a machine learning model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained machine learning model.\n",
    "    - X_test (DataFrame): Test features.\n",
    "    - y_test (Series): Test target.\n",
    "    - start_index (int, optional): Starting index for the data subset. Default is 0.\n",
    "    - end_index (int or None, optional): Ending index for the data subset. Default is None.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a LIME explainer\n",
    "    lime = LimeTabular(model=model.predict_proba, data=X_train, random_state=1)\n",
    "\n",
    "    # Explain the local model for the selected data\n",
    "    lime_local = lime.explain_local(X_test.iloc[start_index:end_index], \n",
    "                                    y_test.iloc[start_index:end_index], \n",
    "                                    name='LIME')\n",
    "\n",
    "    # Show the LIME explanation\n",
    "    show(lime_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d98e26c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_tabular_explainer(model, X_train, instance, num_features):\n",
    "    \"\"\"\n",
    "    Generate and visualize a LIME (Local Interpretable Model-agnostic Explanations) explanation for a tabular dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained machine learning model to be explained.\n",
    "    - X_train: The training data used to train the model. It should be a pandas DataFrame.\n",
    "    - instance_to_explain: Index of the specific instance in X_train that you want to explain.\n",
    "    - num_features: Number of features to include in the explanation.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Create the explainer\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(X_train.values,\n",
    "                                                      class_names=['Not Depressed', 'Depressed'],\n",
    "                                                      feature_names=X_train.columns.tolist()\n",
    "                                                      )\n",
    "    # Choose the specific instance for explanation\n",
    "    instance_to_explain = X_test.iloc[instance]\n",
    "    # Generate the explanation\n",
    "    explanation = explainer.explain_instance(instance_to_explain.values, model.predict_proba, num_features=num_features)\n",
    "    # Visualize the explanation\n",
    "    print(f\"Instance: {instance}\")\n",
    "    explanation.show_in_notebook(show_all=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9db2ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_explanation(model, X_test, start_index=0, end_index=None):\n",
    "    \"\"\"\n",
    "    Generates and displays SHAP explanations for a machine learning model.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained machine learning model.\n",
    "    - X_test (DataFrame): Test features.\n",
    "    - start_index (int, optional): Starting index for the data subset. Default is 0.\n",
    "    - end_index (int or None, optional): Ending index for the data subset. Default is None.\n",
    "    \"\"\"\n",
    "        \n",
    "   # If end_index is None, take all indices from start_index to the end\n",
    "    end_index = end_index or len(X_test)\n",
    "\n",
    "    # Create a SHAP explainer object\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "\n",
    "    # Calculate SHAP values for the selected data\n",
    "    shap_values = explainer.shap_values(X_test.iloc[start_index:end_index])\n",
    "    \n",
    "#     # class 0 = Contribution to class 1\n",
    "#     # class 1 = Contribution to class 2\n",
    "#     print(shap_values[0].shape)\n",
    "#     print(shap_values)\n",
    "\n",
    "    # Display the SHAP force plot\n",
    "    prediction = model.predict(X_test[start_index:end_index])[0]\n",
    "    print(f\"Prediction: {prediction}\")\n",
    "    \n",
    "    if (end_index - start_index == 1):\n",
    "        display(shap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[start_index:end_index],matplotlib=matplotlib))  \n",
    "    else:\n",
    "         display(shap.force_plot(explainer.expected_value[1], shap_values[1], X_test.iloc[start_index:end_index],matplotlib=False))  \n",
    "\n",
    "    display(shap.summary_plot(shap_values,X_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
